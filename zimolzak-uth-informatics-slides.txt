---
title: Machine Learning to Enhance Electronic Detection of Diagnostic Errors
author: Andrew Zimolzak, MD, MMSc
institute:
- Department of Medicine, Section of Health Services Research
- Baylor College of Medicine
- Michael E.\ DeBakey VA Medical Center
date: November 19, 2025
theme: Goettingen
fonttheme: structurebold
colortheme: whale
aspectratio: 149
---

# Introduction

## Background: electronic triggers (e-triggers)

People have studied management errors. Example.

- E-triggers: rules-based algorithms using EHR data to flag possible MODs
- Limitations: low predictive value; built from *a priori* rules, may miss signals
- Opportunity: augment with data-driven ML


## Diagnostic process[^nasem]

![diagnostic process](img/dxprocess.png)\

[^nasem]: National Academies of Sciences, Engineering, and Medicine
2015. *Improving Diagnosis in Health Care.* Washington, DC: The
National Academies Press. https://doi.org/10.17226/21794.


## Background: problem

- Diagnostic errors cause substantial patient harm
- Monitoring strategies are underdeveloped
- Manual chart review is time-consuming and not scalable


## Electronic detection 2

FIXME


## Electronic detection 3

FIXME


## Objectives

![paper title](img/papertitle.png)\

- Determine whether ML can enhance e-trigger performance to identify missed opportunities in diagnosis (MODs)
- Emulate human reviewers at larger scale. (*Detect* possible MOD afterwards. *Not* predict MOD in the ED!)


# Methods



## Study design: overview

- Retrospective cohort analysis using VA national EHR (>20M individuals)
- Two high-risk ED cohorts identified by rules-based e-triggers
- Expert clinician review provided criterion labels (MOD vs no MOD)
- ML models trained and tested on structured EHR variables
- Flow of EHR $\to$ e-trigger $\to$ reviewer


## Cohorts: dizziness/stroke risk

- Inclusion: ED visits for dizziness/vertigo + stroke risk factors
- Outcome: hospitalization for stroke or TIA within 30 days after ED discharge
- Timeframe: 2016--2020


## Cohorts: abdominal pain/fever

- Inclusion: ED visits for abdominal pain + abnormal temperature
- Outcome: hospitalization within 10 days after ED discharge
- Examples of missed diagnoses: cholangitis, cholecystitis, infectious colitis


## Data sources & labeling

- Data: Structured EHR from index ED visit and subsequent hospital data
- Random sample of trigger-positive records reviewed by trained clinicians using standardized instrument
- Labeled records split into training and test sets
- IRB approval; waiver of consent


## Variables used for ML

- Dizziness model: 148 candidate variables
- Abdominal pain model: 153 candidate variables
- Types: demographics, vitals, labs, orders, visit times, risk factors
- Preselection via bivariate tests (t-test or $\chi^2$), $P < 0.10$


## Machine learning methods

- Algorithms: regularized logistic regression and random forest
- Random forest with limited tree depth to reduce overfitting
- Tools: Python 3.7.4; `scipy`, `numpy`, `scikit-learn`
- Performance metric focus: positive predictive value (PPV) with 95% Wald CI


# Results

## Label counts: dizziness cohort

- Rules-based flagged: 82 reviewed records
- Reviewer-identified MODs: 39/82 (PPV 48%; 95% CI 37--58)
- [Image placeholder: small bar chart]


## ML results: dizziness cohort

- Best ML (random forest) performance:
  - Correctly identified 36/39 true MODs
  - Correctly identified 40/43 negatives
  - PPV 92% (95% CI 84--100)
- ML reduced manual review burden substantially
- [Image placeholder: confusion matrix sketch]


## Label counts: abdominal pain cohort

- Rules-based flagged: 104 reviewed records
- Reviewer-identified MODs: 31/104 (PPV 30%; 95% CI 21--39)
- [Image placeholder: small bar chart]


## ML results: abdominal pain cohort

- ML correctly identified 26/31 true MODs and 71/73 negatives
- Reported PPV 93% (95% CI 83--100)
- [Image placeholder: confusion matrix sketch]


## Comparative table: rules vs ML (summary)

E-trigger            | True+/Total | PPV (CI)
---------------------|:-----------:|---------
**Dizziness**                | |
\quad{} Rules-based positive for MOD | 39/82  | **48%** (37--58)
\quad{} ML positive for MOD          | 36/39  | **92%** (84--100)
\quad{} ML negative for MOD          | 3/43   | NA
**Abdominal pain**           | |
\quad{} Rules-based positive for MOD | 31/104 | **30%** (21--39)
\quad{} ML positive for MOD          | 26/28  | **93%** (83--100)
\quad{} ML negative for MOD          | 5/76   | NA


## Example study flow diagram

![study flow](img/studyflow-crop.png)\


# Conclusions

## Interpretation: key findings

- ML substantially increased PPV of e-triggers for both cohorts
- ML models approximated clinician labels with high accuracy
- Enables more efficient, scalable monitoring of diagnostic errors


## Practical implications

- Health systems can use ML-enhanced e-triggers to:
  - Prioritize cases for chart review
  - Monitor diagnostic safety at scale
  - Support quality improvement and research


## Future directions

- Incorporate clinical note text (NLP) to capture richer signals
- Increase expert-labeled training set size
- Validate models in external, independent populations
- Explore implementation workflows for real-time monitoring


## Takeaway/Conclusions

- ML can markedly improve rules-based e-triggers for detecting missed diagnostic opportunities
- Offers scalable approach to identify diagnostic errors and reduce manual review burden
- Promising tool for research and quality improvement, pending broader validation


# Limitations

## Limitations

- Time/resource cost to prepare structured variables
- Small number of expert-labeled records limits model learning and test precision
- Models used only structured data: may miss signals in clinical notes
- Single-system (VA) data; requires external validation


## Lim 2

FIXME


## LIM 3

FIXME


# Technical challenges

## CHAL 1

FIXME


## CHAL 2

FIXME


## CHAL 3

FIXME


# Other BCM Informatics

## Appendix/Methods details (optional)

- Statistical thresholds: bivariate preselection $P < 0.10$
- Models trained on clinician-labeled gold standard; PPVs pooled across train/test due to small sample
- CIs computed as 95% Wald intervals


## Acknowledgments & funding

- VA data infrastructure and clinician reviewers
- Baylor College of Medicine IRB approval; waiver of consent
- Analysis timeframe: Apr 2020--May 2024


## Questions

- Contact/corresponding author info (add as needed)


## 32

FIXME


## 33

FIXME
