---
title: Machine Learning to Enhance Electronic Detection of Diagnostic Errors
author: Andrew Zimolzak, MD, MMSc
institute:
- Department of Medicine, Section of Health Services Research
- Baylor College of Medicine
- Michael E.\ DeBakey VA Medical Center
date: November 19, 2025
theme: Goettingen
fonttheme: structurebold
colortheme: whale
aspectratio: 149
---

# Introduction

## Background: electronic triggers (e-triggers)

People have studied management errors. Example.[^mgt]

[^mgt]: FIXME

- What about diagnosis?


## Diagnostic process[^national]

![diagnostic process](img/dxprocess.png)\

[^national]: National Academies of Sciences, Engineering, and Medicine
2015. *Improving Diagnosis in Health Care.* Washington, DC: The
National Academies Press. https://doi.org/10.17226/21794.


## Background: problem

- Diagnostic errors cause substantial patient harm

- Monitoring strategies[^dxbasic] are underdeveloped

- Manual chart review is time-consuming and not scalable

[^dxbasic]: FIXME reference 1


## Diagnostic e-triggers

E-triggers:
: Rules-based algorithms using EHR data to flag possible MODs

- Limitations: low predictive value; built from *a priori* rules, may
  miss signals

- Opportunity: Augment with data-driven ML


## Electronic detection (more)

AI option[^aimod]

[^aimod]: FIXME ref 2


## Objectives

![paper title](img/papertitle.png)\

- Determine whether ML can enhance e-trigger performance to identify
  missed opportunities in diagnosis (MODs)

- Emulate human reviewers at larger scale. (*Detect* possible MOD
  afterwards. *Not* predict MOD in the ED!)


# Methods

## Study design: overview

- Retrospective cohort analysis using VA national EHR (>20M
  patients)

- Two high-risk ED cohorts identified by rules-based e-triggers

- Expert clinician review provided criterion labels (MOD vs no MOD)

- ML models trained and tested on structured EHR variables

- Flow: EHR $\to$ e-trigger $\to$ reviewer


## E-trigger 1: dizziness + stroke risk

- Inclusion: ED visits for dizziness/vertigo + stroke risk factors

- Outcome: hospitalization for stroke or TIA within 30 days after ED
  discharge

- Timeframe: 2016--2020


## E-trigger 2: abdominal pain + vitals

- Inclusion: ED visits for abdominal pain + abnormal temperature

- Outcome: hospitalization within 10 days after ED discharge

- Examples of missed diagnoses: cholangitis, cholecystitis, infectious
  colitis


## Data sources & labeling

- Data: Structured EHR from index ED visit and subsequent hospital
  data

- Random sample of trigger-positive records reviewed by trained
  clinicians using standardized instrument

- Labeled records split into training and test sets

- IRB approval; waiver of consent


## Variables used for ML

- Dizziness model: 148 candidate variables

- Abdominal pain model: 153 candidate variables

- Types: demographics, vitals, labs, orders, visit times, risk factors

- Preselection via bivariate tests (t-test or $\chi^2$), $P < 0.10$


## Machine learning methods

- Algorithms: regularized logistic regression and random forest

- Random forest with limited tree depth to reduce overfitting

- Tools: Python 3.7.4; `scipy`, `numpy`, `scikit-learn`

- Performance metrics: positive predictive value (PPV) with 95% Wald
  CI


# Results

## Label counts: dizziness cohort

- Rules-based flagged: 82 reviewed records

- Reviewer-identified MODs: 39/82 (PPV 48%; 95% CI 37--58)


## ML results: dizziness cohort

- Best ML (random forest) performance:

  - Correctly identified 36/39 true MODs
  
  - Correctly identified 40/43 negatives
  
  - PPV 92% (95% CI 84--100)

- ML reduced manual review burden substantially


## Label counts: abdominal pain cohort

- Rules-based flagged: 104 reviewed records

- Reviewer-identified MODs: 31/104 (PPV 30%; 95% CI 21--39)


## ML results: abdominal pain cohort

- ML correctly identified 26/31 true MODs and 71/73 negatives

- Reported PPV 93% (95% CI 83--100)


## Comparative table: rules vs ML (summary)

E-trigger            | True+/Total | PPV (CI)
---------------------|:-----------:|---------
**Dizziness**                | |
\quad{} Rules-based positive for MOD | 39/82  | **48%** (37--58)
\quad{} ML positive for MOD          | 36/39  | **92%** (84--100)
\quad{} ML negative for MOD          | 3/43   | NA
**Abdominal pain**           | |
\quad{} Rules-based positive for MOD | 31/104 | **30%** (21--39)
\quad{} ML positive for MOD          | 26/28  | **93%** (83--100)
\quad{} ML negative for MOD          | 5/76   | NA


## Example study flow diagram

![study flow](img/studyflow-crop.png)\


# Conclusions

## Interpretation: key findings

- ML substantially increased PPV of e-triggers for both cohorts

- ML models approximated clinician labels with high accuracy

- Enables more efficient, scalable monitoring of diagnostic errors


## Practical implications

Health systems can use ML-enhanced e-triggers to:

- Prioritize cases for chart review

- Monitor diagnostic safety at scale

- Support quality improvement and research


## Future directions

- Incorporate clinical note text (NLP) to capture richer signals

- Increase expert-labeled training set size

- Validate models in external, independent populations

- Explore implementation workflows for real-time monitoring


## Takeaway/Conclusions

- ML can markedly improve rules-based e-triggers for detecting missed
  diagnostic opportunities

- Offers scalable approach to identify diagnostic errors and reduce
  manual review burden

- Promising tool for research and quality improvement, pending broader
  validation


# Limitations

## Limitations

- Time/resource cost to prepare structured variables

- Small number of expert-labeled records limits model learning and
  test precision

- Models used only structured data: may miss signals in clinical notes

- Single-system (VA) data; requires external validation


## Lim 2

FIXME


## LIM 3

FIXME


# Technical challenges

## Challenges 1

FIXME


## Challenges 2

FIXME


## Challenges 3

FIXME


# Other BCM Informatics

## Appendix/Methods details (optional)

- Statistical thresholds: bivariate preselection $P < 0.10$

- Models trained on clinician-labeled gold standard; PPVs pooled
  across train/test due to small sample

- CIs computed as 95% Wald intervals


## Funding

Agency for Healthcare Research and Quality: **R01 HS027363**

Partial funding:

- Houston VA HSR&D Center for Innovations in Quality, Effectiveness and Safety (CIN 13-413)

- VA National Center for Patient Safety

- AHRQ R01 HS028595

- AHRQ R18 HS029347





## Contact

zimolzak@bcm.edu

github.com/zimolzak/ml-detect-dx-error-seminar

Coauthors:

- Li Wei
- Usman Mir
- Ashish Gupta
- Viralkumar Vaghani
- Adel Hassan
- Devika Subramanian (Rice Univ.)
    - Max Yu
    - Angela Wu
    - Justin Mower
- Hardeep Singh
